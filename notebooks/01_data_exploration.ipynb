{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Financial News Analysis - Exploratory Data Analysis\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Import custom analysis modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path.cwd().parent))  # Add project root to sys.path\n",
        "\n",
        "# You can import full modules if the script contains functions\n",
        "import src.text_processing as text_processing\n",
        "import src.temporal_analysis as temporal_analysis\n",
        "import src.market_analysis as market_analysis\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "# Configure plotting (avoid deprecated seaborn style)\n",
        "sns.set_theme()\n",
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('../data/raw/raw_analyst_ratings.csv')\n",
        "\n",
        "# Basic dataset information\n",
        "print(\"\\nDataset Overview:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Number of records: {len(df):,}\")\n",
        "print(f\"Number of columns: {len(df.columns)}\")\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "\n",
        "# Display data types and missing values\n",
        "print(\"\\nData Types and Missing Values:\")\n",
        "print(\"-\" * 50)\n",
        "info_df = pd.DataFrame({\n",
        "    'dtype': df.dtypes,\n",
        "    'missing': df.isnull().sum(),\n",
        "    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "print(info_df)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nSample Data:\")\n",
        "print(\"-\" * 50)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"Number of records: {len(df):,}\")\n",
        "print(f\"Number of columns: {len(df.columns)}\")\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "\n",
        "# Display data types and missing values\n",
        "print(\"\\nData Types and Missing Values:\")\n",
        "print(\"-\" * 50)\n",
        "info_df = pd.DataFrame({\n",
        "    'dtype': df.dtypes,\n",
        "    'missing': df.isnull().sum(),\n",
        "    'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "print(info_df)\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nSample Data:\")\n",
        "print(\"-\" * 50)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Descriptive Statistics - Headlines\n",
        "print(\"Headline Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Calculate headline statistics\n",
        "df['headline_length'] = df['headline'].str.len()\n",
        "df['word_count'] = df['headline'].str.split().str.len()\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nHeadline Statistics:\")\n",
        "print(df[['headline_length', 'word_count']].describe())\n",
        "\n",
        "# Create visualizations\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Headline length distribution\n",
        "sns.histplot(data=df, x='headline_length', bins=50, ax=ax1)\n",
        "ax1.set_title('Distribution of Headline Lengths')\n",
        "ax1.set_xlabel('Number of Characters')\n",
        "ax1.set_ylabel('Count')\n",
        "\n",
        "# Word count distribution\n",
        "sns.histplot(data=df, x='word_count', bins=30, ax=ax2)\n",
        "ax2.set_title('Distribution of Words per Headline')\n",
        "ax2.set_xlabel('Number of Words')\n",
        "ax2.set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display example headlines\n",
        "print(\"\\nExample Headlines:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"\\nShortest Headlines:\")\n",
        "print(df.nsmallest(3, 'headline_length')[['headline', 'headline_length', 'word_count']])\n",
        "print(\"\\nLongest Headlines:\")\n",
        "print(df.nlargest(3, 'headline_length')[['headline', 'headline_length', 'word_count']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Publisher Analysis\n",
        "print(\"Publisher Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Calculate publisher statistics\n",
        "publisher_stats = df['publisher'].value_counts()\n",
        "publisher_pct = (publisher_stats / len(df) * 100).round(2)\n",
        "\n",
        "# Create DataFrame with publisher statistics\n",
        "publisher_df = pd.DataFrame({\n",
        "    'Article_Count': publisher_stats,\n",
        "    'Percentage': publisher_pct\n",
        "})\n",
        "\n",
        "print(\"\\nTop 10 Publishers:\")\n",
        "print(publisher_df.head(10))\n",
        "\n",
        "# Visualize publisher distribution\n",
        "plt.figure(figsize=(15, 6))\n",
        "publisher_stats.head(15).plot(kind='bar')\n",
        "plt.title('Top 15 Publishers by Number of Articles')\n",
        "plt.xlabel('Publisher')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Publisher diversity statistics\n",
        "print(\"\\nPublisher Statistics:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Total unique publishers: {len(publisher_stats):,}\")\n",
        "print(f\"Average articles per publisher: {publisher_stats.mean():.2f}\")\n",
        "print(f\"Median articles per publisher: {publisher_stats.median():.2f}\")\n",
        "\n",
        "# Calculate concentration metrics\n",
        "top_10_pct = publisher_pct.head(10).sum()\n",
        "print(f\"\\nTop 10 publishers account for {top_10_pct:.2f}% of all articles\")\n",
        "\n",
        "# Check for email addresses in publisher names\n",
        "email_publishers = df['publisher'].str.contains('@', na=False)\n",
        "if email_publishers.any():\n",
        "    print(\"\\nPublishers using email addresses:\")\n",
        "    print(df[email_publishers]['publisher'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text Analysis: Keyword Frequency from Headlines\n",
        "\n",
        "print(\"Text Analysis - Keyword Frequency\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Ensure NLTK resources are available\n",
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define stop words and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Tokenize and clean headlines\n",
        "def clean_and_tokenize(text):\n",
        "    tokens = text.lower().split()\n",
        "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Apply tokenization\n",
        "df['tokens'] = df['headline'].astype(str).apply(clean_and_tokenize)\n",
        "\n",
        "# Flatten all tokens\n",
        "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
        "\n",
        "# Count keyword frequency\n",
        "word_freq = Counter(all_tokens)\n",
        "top_keywords = pd.DataFrame(word_freq.most_common(20), columns=['Keyword', 'Frequency'])\n",
        "\n",
        "# Plot top keywords\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=top_keywords, x='Keyword', y='Frequency')\n",
        "plt.title('Top 20 Keywords in Headlines')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display top keywords\n",
        "print(\"\\nMost Common Keywords:\")\n",
        "display(top_keywords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publisher Analysis - Frequency Count and Domain Analysis\n",
        "\n",
        "print(\"Publisher Analysis - Frequency and Email Domains\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Top publishers by count\n",
        "publisher_counts = df['publisher'].value_counts()\n",
        "print(\"\\nTop 10 Publishers by Article Count:\")\n",
        "display(publisher_counts.head(10))\n",
        "\n",
        "# Extract domain from email-like publishers\n",
        "def extract_domain(publisher):\n",
        "    if '@' in publisher:\n",
        "        return publisher.split('@')[-1].lower()\n",
        "    return None\n",
        "\n",
        "df['publisher_domain'] = df['publisher'].apply(extract_domain)\n",
        "\n",
        "# Count domains (excluding None)\n",
        "domain_counts = df['publisher_domain'].dropna().value_counts()\n",
        "\n",
        "print(\"\\nTop Email Domains (from publishers):\")\n",
        "display(domain_counts.head(10))\n",
        "\n",
        "# Optional: plot domain counts\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=domain_counts.head(10).index, y=domain_counts.head(10).values)\n",
        "plt.title('Top 10 Publisher Email Domains')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Domain')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Temporal Analysis\n",
        "print(\"Temporal Analysis\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Parse dates robustly\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# Check for unparseable dates\n",
        "num_invalid = df['date'].isna().sum()\n",
        "if num_invalid > 0:\n",
        "    print(f\"⚠️ Warning: {num_invalid} unparseable date values found and set as NaT.\")\n",
        "\n",
        "# Drop rows with invalid dates if necessary\n",
        "df = df.dropna(subset=['date'])\n",
        "\n",
        "# Extract temporal components\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day'] = df['date'].dt.day\n",
        "df['hour'] = df['date'].dt.hour\n",
        "df['day_of_week'] = df['date'].dt.day_name()\n",
        "df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday'])\n",
        "\n",
        "# Calculate various time-based aggregations\n",
        "daily_counts = df.groupby(df['date'].dt.date).size()\n",
        "hourly_counts = df.groupby('hour').size()\n",
        "dow_counts = df.groupby('day_of_week').size()\n",
        "monthly_counts = df.groupby([df['year'], df['month']]).size()\n",
        "\n",
        "# Print temporal statistics\n",
        "print(\"\\nTemporal Coverage:\")\n",
        "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Total days covered: {len(daily_counts)}\")\n",
        "print(f\"Average daily articles: {daily_counts.mean():.2f}\")\n",
        "print(f\"Peak hour for publications: {hourly_counts.idxmax()}:00\")\n",
        "print(f\"Weekend publication ratio: {df['is_weekend'].mean():.2%}\")\n",
        "\n",
        "# Create visualizations\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Daily volume\n",
        "daily_counts.plot(ax=ax1)\n",
        "ax1.set_title('Daily News Volume')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Number of Articles')\n",
        "\n",
        "# Hourly patterns\n",
        "hourly_counts.plot(kind='bar', ax=ax2)\n",
        "ax2.set_title('Articles by Hour of Day')\n",
        "ax2.set_xlabel('Hour (24-hour format)')\n",
        "ax2.set_ylabel('Number of Articles')\n",
        "\n",
        "# Day of week patterns\n",
        "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "dow_counts = dow_counts.reindex(dow_order)\n",
        "dow_counts.plot(kind='bar', ax=ax3)\n",
        "ax3.set_title('Articles by Day of Week')\n",
        "ax3.set_xlabel('Day of Week')\n",
        "ax3.set_ylabel('Number of Articles')\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Monthly trends\n",
        "monthly_counts.plot(ax=ax4)\n",
        "ax4.set_title('Monthly Article Volume')\n",
        "ax4.set_xlabel('Year-Month')\n",
        "ax4.set_ylabel('Number of Articles')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify high-volume days\n",
        "print(\"\\nTop 5 Days by Article Volume:\")\n",
        "print(daily_counts.nlargest(5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "src_path = str(Path.cwd().parent / 'src')\n",
        "if src_path not in sys.path:\n",
        "    sys.path.append(src_path)\n",
        "\n",
        "# Import our utility modules\n",
        "import text_processing\n",
        "import temporal_analysis\n",
        "import market_analysis\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme()\n",
        "plt.style.use('default')\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_theme()  # This sets both the style and palette\n",
        "plt.style.use('default')  # Use default matplotlib style as base\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('../data/raw/raw_analyst_ratings.csv')\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nData Types:\\n\", df.dtypes)\n",
        "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate headline statistics\n",
        "df['headline_length'] = df['headline'].str.len()\n",
        "df['word_count'] = df['headline'].str.split().str.len()\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"Headline Statistics:\")\n",
        "print(df[['headline_length', 'word_count']].describe())\n",
        "\n",
        "# Create plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot headline length distribution\n",
        "sns.histplot(data=df, x='headline_length', bins=50, ax=ax1)\n",
        "ax1.set_title('Distribution of Headline Lengths')\n",
        "ax1.set_xlabel('Number of Characters')\n",
        "ax1.set_ylabel('Count')\n",
        "\n",
        "# Plot word count distribution\n",
        "sns.histplot(data=df, x='word_count', bins=30, ax=ax2)\n",
        "ax2.set_title('Distribution of Word Counts in Headlines')\n",
        "ax2.set_xlabel('Number of Words')\n",
        "ax2.set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display some example headlines of different lengths\n",
        "print(\"\\nExample Headlines:\")\n",
        "print(\"\\nShortest Headlines:\")\n",
        "display(df.nsmallest(3, 'headline_length')[['headline', 'headline_length', 'word_count']])\n",
        "print(\"\\nLongest Headlines:\")\n",
        "display(df.nlargest(3, 'headline_length')[['headline', 'headline_length', 'word_count']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries for text processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download all required NLTK data\n",
        "print(\"Downloading required NLTK data...\")\n",
        "nltk.download(['punkt', 'stopwords', 'averaged_perceptron_tagger'])\n",
        "\n",
        "# Function to process text\n",
        "def process_text(text):\n",
        "    try:\n",
        "        # Convert to string if not already\n",
        "        text = str(text)\n",
        "        # Tokenize\n",
        "        tokens = text.lower().split()  # Using simple split instead of word_tokenize for robustness\n",
        "        # Remove stopwords and punctuation\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "        return tokens\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {e}\")\n",
        "        return []\n",
        "\n",
        "# Process all headlines\n",
        "all_tokens = []\n",
        "for headline in df['headline']:\n",
        "    all_tokens.extend(process_text(headline))\n",
        "\n",
        "# Get most common words\n",
        "word_freq = Counter(all_tokens)\n",
        "most_common_words = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Frequency'])\n",
        "\n",
        "# Plot most common words\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=most_common_words, x='Word', y='Frequency')\n",
        "plt.title('20 Most Common Words in Headlines')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display the most common words\n",
        "print(\"\\nMost Common Words:\")\n",
        "display(most_common_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine our date data\n",
        "print(\"Data type of date column:\", df['date'].dtype)\n",
        "print(\"\\nSample dates from the dataset:\")\n",
        "print(df['date'].head())\n",
        "print(\"\\nUnique date formats (first 5):\")\n",
        "print(df['date'].drop_duplicates().head())\n",
        "\n",
        "# Try converting with error handling\n",
        "try:\n",
        "    # First attempt with default parser\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "except Exception as e:\n",
        "    print(\"\\nError in default parsing:\", str(e))\n",
        "    try:\n",
        "        # Second attempt with coerce option to handle errors\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        print(\"\\nConverted dates with 'coerce' option:\")\n",
        "        print(df['date'].head())\n",
        "\n",
        "        # Check for any NaT (Not a Time) values\n",
        "        nat_count = df['date'].isna().sum()\n",
        "        if nat_count > 0:\n",
        "            print(f\"\\nWarning: {nat_count} dates could not be parsed\")\n",
        "    except Exception as e:\n",
        "        print(\"\\nError in coerced parsing:\", str(e))\n",
        "\n",
        "print(\"\\nFinal data type of date column:\", df['date'].dtype)\n",
        "\n",
        "# Extract various time components\n",
        "df['year'] = df['date'].dt.year\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day_of_week'] = df['date'].dt.day_name()\n",
        "df['hour'] = df['date'].dt.hour\n",
        "\n",
        "# Create subplots for different temporal patterns\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Articles by month\n",
        "monthly_counts = df['month'].value_counts().sort_index()\n",
        "sns.barplot(x=monthly_counts.index, y=monthly_counts.values, ax=ax1)\n",
        "ax1.set_title('Articles by Month')\n",
        "ax1.set_xlabel('Month')\n",
        "ax1.set_ylabel('Number of Articles')\n",
        "\n",
        "# 2. Articles by day of week\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "daily_counts = df['day_of_week'].value_counts()\n",
        "daily_counts = daily_counts.reindex(day_order)\n",
        "sns.barplot(x=daily_counts.index, y=daily_counts.values, ax=ax2)\n",
        "ax2.set_title('Articles by Day of Week')\n",
        "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
        "ax2.set_ylabel('Number of Articles')\n",
        "\n",
        "# 3. Articles by hour\n",
        "hourly_counts = df['hour'].value_counts().sort_index()\n",
        "sns.barplot(x=hourly_counts.index, y=hourly_counts.values, ax=ax3)\n",
        "ax3.set_title('Articles by Hour of Day')\n",
        "ax3.set_xlabel('Hour')\n",
        "ax3.set_ylabel('Number of Articles')\n",
        "\n",
        "# 4. Articles over time (daily)\n",
        "daily_articles = df.groupby(df['date'].dt.date).size()\n",
        "daily_articles.plot(ax=ax4)\n",
        "ax4.set_title('Articles Over Time')\n",
        "ax4.set_xlabel('Date')\n",
        "ax4.set_ylabel('Number of Articles')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\nTemporal Distribution Summary:\")\n",
        "print(\"\\nTop 5 Most Active Days:\")\n",
        "display(daily_articles.nlargest(5))\n",
        "\n",
        "print(\"\\nAverage Articles per:\")\n",
        "print(f\"Day: {daily_articles.mean():.2f}\")\n",
        "print(f\"Week Day: {daily_counts.mean():.2f}\")\n",
        "print(f\"Hour: {hourly_counts.mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze publishers\n",
        "publisher_counts = df['publisher'].value_counts()\n",
        "\n",
        "# Display basic publisher statistics\n",
        "print(\"Publisher Statistics:\")\n",
        "print(f\"Total number of unique publishers: {len(publisher_counts)}\")\n",
        "print(f\"Average articles per publisher: {publisher_counts.mean():.2f}\")\n",
        "print(f\"Median articles per publisher: {publisher_counts.median():.2f}\")\n",
        "\n",
        "# Plot top publishers\n",
        "plt.figure(figsize=(12, 6))\n",
        "publisher_counts.head(15).plot(kind='bar')\n",
        "plt.title('Top 15 Publishers by Number of Articles')\n",
        "plt.xlabel('Publisher')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display top publishers and their article counts\n",
        "print(\"\\nTop 15 Publishers:\")\n",
        "display(pd.DataFrame({\n",
        "    'Publisher': publisher_counts.head(15).index,\n",
        "    'Number of Articles': publisher_counts.head(15).values,\n",
        "    'Percentage of Total': (publisher_counts.head(15).values / len(df) * 100).round(2)\n",
        "}))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze stock symbols\n",
        "stock_counts = df['stock'].value_counts()\n",
        "\n",
        "# Display basic stock statistics\n",
        "print(\"Stock Statistics:\")\n",
        "print(f\"Total number of unique stocks: {len(stock_counts)}\")\n",
        "print(f\"Average articles per stock: {stock_counts.mean():.2f}\")\n",
        "print(f\"Median articles per stock: {stock_counts.median():.2f}\")\n",
        "\n",
        "# Plot top stocks\n",
        "plt.figure(figsize=(12, 6))\n",
        "stock_counts.head(15).plot(kind='bar')\n",
        "plt.title('Top 15 Most Mentioned Stocks')\n",
        "plt.xlabel('Stock Symbol')\n",
        "plt.ylabel('Number of Articles')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display top stocks and their article counts\n",
        "print(\"\\nTop 15 Most Covered Stocks:\")\n",
        "display(pd.DataFrame({\n",
        "    'Stock Symbol': stock_counts.head(15).index,\n",
        "    'Number of Articles': stock_counts.head(15).values,\n",
        "    'Percentage of Total': (stock_counts.head(15).values / len(df) * 100).round(2)\n",
        "}))\n",
        "\n",
        "# Analyze coverage distribution\n",
        "coverage_stats = pd.DataFrame({\n",
        "    'Articles': stock_counts.value_counts().sort_index()\n",
        "})\n",
        "coverage_stats['Stocks'] = coverage_stats.index\n",
        "coverage_stats['Cumulative Stocks'] = coverage_stats['Articles'].cumsum()\n",
        "coverage_stats['Percentage of Stocks'] = (coverage_stats['Cumulative Stocks'] / len(stock_counts) * 100).round(2)\n",
        "\n",
        "print(\"\\nCoverage Distribution:\")\n",
        "print(\"Number of stocks with:\")\n",
        "for articles in [1, 5, 10, 50, 100]:\n",
        "    stocks_above = len(stock_counts[stock_counts >= articles])\n",
        "    percentage = (stocks_above / len(stock_counts) * 100)\n",
        "    print(f\"{articles}+ articles: {stocks_above} stocks ({percentage:.2f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
